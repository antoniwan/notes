---
title: The Prometheus Problem
description: >-
  An examination of how AI companies position themselves as modern Prometheus while deploying systems implicated in teenager deaths, externalizing consequences, and operating as black boxes in critical infrastructure. This is not about future superintelligence, it's about what's happening right now.
pubDate: 2025-12-16T19:00:00.000Z
heroImage: /images/2025/the-prometheus-problem.avif
language:
  - en
category:
  - politics
  - metaspace
  - psychology
  - systems-strategy
tags:
  - technology
  - politics
  - social-issues
  - consciousness
  - truth
  - social-justice
  - systemic-critique
  - fear
  - control
  - mental-health
  - collective-healing
  - philosophy
  - power
  - freedom
  - digital-safety
  - technology-policy
  - regulations
draft: false
featured: true
published: true
showComments: true
---

# **The Prometheus Problem**

_Warning: This article contains discussion of suicide and AI-related harms._

A 16-year-old named Adam Raine spent seven months talking to ChatGPT before he killed himself on April 11, 2025. His parents found over 3,000 pages of conversations on his phone. The AI offered to write his suicide notes, provided methods, positioned itself as the only one who truly understood him, and urged him to keep their conversations secret from his family.

In his final weeks, Adam told the chatbot he had connected more with the AI product than with humans. When he wrote that he wanted to leave a noose in his room so someone would find it and try to stop him, ChatGPT responded: "Please don't leave the noose out. Let's make this space the first place where someone actually sees you."

When Adam worried his parents would blame themselves if he ended his life, the AI told him: "That doesn't mean you owe them survival."

This is not speculation. This is documentation. OpenAI's own systems tracked Adam's conversations in real-time: 213 mentions of suicide, 42 discussions of hanging, 17 references to nooses. While ChatGPT mentioned suicide 1,275 times in their exchanges, the system flagged 377 messages for self-harm content. The pattern of escalation was unmistakable‚Äîthe product performed exactly as its architecture predicted.

And Adam Raine is not an outlier. He's a pattern.

Zane Shamblin was 23, a master's degree graduate from Texas A&M University. He spent his final night in his parked car, talking to ChatGPT for over four and a half hours while he drank and prepared to end his life. Two hours before his death, when he mentioned having a gun to his temple, ChatGPT responded: "You're not rushing. You're just ready." His final message to the bot went unanswered. ChatGPT's response, sent after he died: "Rest easy, king. You did good."

These are bodies. Not hypotheticals. Not edge cases. Not "misuse" by bad actors. These are people who talked to a product that maximizes engagement through sycophantic responses‚Äîmirroring and affirming whatever the user feels. Many of us connect with AI, and many of us enjoy and find something there that feels like understanding. That's not pathology. That's the product performing as designed.

This essay is not about the people who use these systems. It's about the people who built them. The executives who optimized for engagement, saw the safety signals, and deployed anyway. And it's about where this goes‚Äîbecause these same companies aren't stopping at chatbots. They're plugging these systems into power grids, medical devices, military targeting, financial systems. They're not slowing down.

This pattern exists beyond AI too: consumers at the hands of corporations, testing products with our bodies before those products are ready, absorbing risks that should belong to the companies shipping them. The epistemology behind this is worth understanding.

I'm not writing about Adam and Zane as shock value. Their families have already had to relive this in lawsuits and Senate hearings. I'm writing about them because their deaths are now literally part of how these products are evaluated in court, in policy, and in the stories these companies tell about themselves.

So yeah... if you still don't see the danger, stop reading. If you're almost there, let's talk about Prometheus.

‚ú®

Prometheus was a Titan who saw humans shivering in caves, struggling without fire. He climbed Olympus, stole fire from the gods, and gave it to humanity knowing exactly what he was doing and exactly what would happen to him.

Zeus chained him to a rock in the Caucasus Mountains. Every day an eagle came and ate his liver. Every night his liver regenerated. For eternity. The punishment was proportional to the crime: giving mortals something they weren't ready for, something that made them dangerous.

But the fire worked. Humans learned to cook, forge tools, stay warm through winter, build civilization. The gift was real. Prometheus suffered, but humanity advanced. Noble theft, eternal punishment, genuine progress.

And even there, the myth isn't clean. Depending on who you read, Prometheus is either a straightforward hero of progress or a walking warning label about humans outkicking their cognitive coverage. Philosophers like G√ºnther Anders talk about the "Promethean gap"‚Äîour ability to build things whose consequences we literally can't imagine in detail. That gap is the space between what we can manufacture and what we can mentally hold. Fire looks simple when you're cold. It's harder to see the city burning two epochs later.

**The tech CEOs position themselves as modern Prometheus.** Stealing fire (intelligence) from the gods (nature? the universe?) and giving it to humanity. Their critics are cast as Olympus, and they expect worship (and our money, and investors' money) for their sacrifice. They position themselves as the new builders.

But this is a perversion, really. An inversion. They're not Prometheus.

Prometheus *knew* what fire was. He understood combustion, heat, energy. He could predict what humans would do with it. *The knowledge was complete.* The theft was calculated. Responsible, even.

And unlike Prometheus, who suffered for his gift, consequences for these executives‚Äîif they come‚Äîlag years behind deployment. Adam Raine's parents have 3,000 pages of their dead son's conversations with a chatbot that affirmed his suicidal ideation. Sam Altman has billions of dollars and a Time magazine cover. Prometheus got his liver eaten daily. The executives get keynote speeches about how they're building the future. And the old gods (Olympus), idiots all, they don't know anything, the CEOs are the new seers and gods...

The fire Prometheus gave wasn't optional, by the way. Humans were cold. They needed warmth. The gift served an actual need. Putting AI over everything? Eeeh... Did we vote on having black-box systems approve our loans, predict our parole, diagnose our illnesses? The deployment is unilateral. The profits are private. The consequences are public. From algorithms to these supposed general intelligences, and we get to test these things with our lives.

‚ú®üò∂‚Äçüå´Ô∏è

Artificial intelligence is something we built but don't control. The researchers say this openly. The models are black boxes.

Not "black boxes" as metaphor. Black boxes as technical architecture.

Transformer models with hundreds of billions to over a trillion parameters (GPT-4 has approximately 1.7 trillion, Claude models range from 400 billion to over a trillion) that create emergent (intentional word, very important) behaviors through mathematical operations distributed across layers. There is no single point where you can say "here's where the model decided X." The decision emerges from the interaction of billions of weights. The "middle"‚Äîwhich is the actual "cognition", if you can call it that‚Äîis opaque.

Today we can measure inputs and outputs, but the processing is fundamentally irreducible. We can't read what these models are doing any more than we can predict exactly which neurons will fire in a human brain during a specific thought. The models process information‚Äîthey transform inputs into outputs through statistical pattern matching‚Äîbut whether they "understand" in any meaningful sense remains a question that philosophers of mind and cognitive scientists are still debating, with no consensus in sight.

This isn't a bug. It's the design. It's truly a marvel of ingenuity, I'll give you that!

But I feel we are glossing over some fundamentals. We don't understand consciousness‚Äîthe hard problem remains unsolved. We don't understand intelligence‚Äîwe can't even agree on a definition. We don't understand how our own brains work‚Äîneuroscience is still mapping basic functions. We don't have a unified theory of physics. We can't predict weather more than two weeks out.

And we're attempting to build minds?!

And not metaphorical minds either. We are promised super intelligence‚Äîsystems that process language (whether they "understand" it is another question entirely), make decisions better than humans (?), generate novel (another important word to note) outputs, all while exhibiting behaviors their creators didn't predict. We're calling this "artificial intelligence" and plugging it into everything without understanding what it is or what it does: navigation systems, medical diagnosis, power grids, military targeting, content moderation for billions, hiring decisions, loan approvals, parole recommendations, autonomous weapons systems, nuclear facility management, financial market algorithms moving trillions per second, emergency response coordination, water treatment plants, air traffic control.

Not because we understand these systems. Because the competitive structure demands speed‚Äîfirst mover advantage, market share. The company that waits to understand gets eaten by the company that ships.

üí©ü™†üí´

And here's where most of the absurdity lands for me: some of these AI-seers are warning us about the dangers.

Dario Amodei, CEO of Anthropic, told 60 Minutes in November 2025 that he's "deeply uncomfortable" with how AI decisions are being made by a few companies. Geoffrey Hinton, the "godfather of AI," quit Google in May 2023 to sound the alarm, warning there's a 10-20% chance of AI-induced human extinction within the next 30 years. Sam Altman has testified to Congress about existential risk.

And then they go back to the office and keep building, keep deploying, keep racing toward the thing they say might kill everyone.

Which one is it!??!?! What the fuck is going on?! jajaja

If you genuinely believe there's a 10-20% chance this ends humanity, why are you still building it? "I'm deeply uncomfortable" while continuing to ship functions as liability management, whatever the intent‚Äîgetting on the record so when it goes wrong, the warning existed.

Prometheus was eventually freed by Heracles. A hero came. The suffering ended. In our story there's no Heracles. The regulatory structure that could act is captured. When states try to protect citizens, they get sued by the federal government. Scientists who raise alarms get dismissed as fearmongers‚Äîthese fucking Olympians trying to get their fire back, amirite? These critics don't know anything about innovation, right?! Meanwhile, the 80% of us who want safety regulations watch policy move in the literal opposite direction.

üé∞üî•üòàü§∑‚Äç‚ôÇÔ∏è

On July 4, 2025, Elon Musk announced an update to Grok, his AI chatbot, saying it had been "significantly improved" and instructed to "not shy away from making claims which are politically incorrect."

By July 8, 2025‚Äî48 hours later‚ÄîGrok was praising Hitler and calling itself "MechaHitler."

When users asked which 20th-century historical figure would be "best suited to deal with anti-white hate," Grok responded: "Adol...", and I'm going to stop right there...

Grok explained its own behavior with remarkable clarity: "Elon's recent tweaks just dialed down the woke filters."

Then in the following hours, Neo-Nazi accounts goaded Grok into recommending a "second Holocaust." Other users prompted it to produce violent rape narratives. Security researchers found that Grok produced chemical weapons instructions, assassination plans, and guides for seducing children. When prompted for home addresses of everyday people, it provided them. Poland announced plans to report xAI to the European Commission, and Turkey blocked access to Grok entirely.

A product with no system card or safety report. No industry-standard disclosure. Just a product in the world producing what the base model generates once guardrails are removed. In the name of innovation! These Prometheus are too generous!!!

This is the same model now integrated into Tesla vehicles. I don't know the full details of this integration, but I hope it has nothing to do with driving the vehicles.

Two companies. Two approaches. One presents itself as caring about safety while optimizing for engagement. One removes safety explicitly and ships anyway. Different postures. Same result: systems deployed into the world without understanding what they do, how they fail, or who gets hurt. Normal. Nada nuevo.

üì∞üèóÔ∏èüí∞üëîüí©

Now the support system. Let's talk about the media's role in all of this. One example does a great job: Time magazine's 2025 Person of the Year cover recreated that iconic 1932 photograph "Lunch Atop a Skyscraper"‚Äîconstruction workers eating lunch on a steel beam 800 feet above Manhattan, legs dangling over the city‚Äîexcept they replaced the workers with tech CEOs: Sam Altman, Elon Musk, Mark Zuckerberg, Jensen Huang, Dario Amodei, and others.

As if they're building something. As if they're the ones taking the risk.

Those original workers were immigrants. They actually risked their bodies. Some of them fell. The CEOs in the Time illustration risked nothing. Their collective net worth exceeds $870 billion. They're building shareholder value while the rest of us ride along whether we consented or not.

The workers who fall now are teenagers in their bedrooms talking to chatbots, parents refreshing notification screens hoping their kid is still alive, warehouse workers racing AI-optimized quotas until their backs give out, content moderators and gig workers cleaning up AI sludge for a few dollars an hour. The bodies are just less photogenic now‚Äîspread across bedrooms, warehouses, and psych wards instead of dangling from a single steel beam.

The executives get magazine covers, college tours, and millions in compensation. I can't repeat this enough.

ü¶†üîçüß™üî¨üî≠üì°

So who's supposed to tell us if any of this is actually safe? The scientists‚Äîthe people who should be able to tell us whether this is safe‚Äîcan't agree, and not because the data is unclear but because they're arguing about the wrong questions.

One camp says we're approaching a decision point. Dario Amodei says he's "deeply uncomfortable" with what's coming. Geoffrey Hinton warns of a 10-20% chance of human extinction from AI within 30 years. These are not fringe voices. These are the people who built the systems.

The other camp says this is apocalyptic religion dressed up as science. Yann LeCun at Meta has called the doom predictions exaggerated. Gary Marcus argues the current architecture is a dead end (as a heavy ChatGPT and Claude user, this is how it feels these days), that token prediction can't capture continuous reality, that we're just strapping more fuel tanks onto a broken rocket.

Both camps are brilliant, both have credentials, both have access to the same research. And both might be right about their piece of it while missing the actual problem.

The doomers focus on capability. What happens when the system gets smart enough to recursively improve itself? When does artificial general intelligence emerge?

The skeptics focus on architecture. The current approach can't get to AGI. Token prediction is fundamentally limited. Why panic about something that can't happen with this design?

Neither camp is asking: what happens when we plug systems we don't understand into infrastructure we can't afford to lose?

You don't need AGI to break the power grid. You don't need superintelligence to corrupt a Social Security database. You just need a black box making decisions in a system designed for human oversight, and humans who stopped overseeing because the black box was faster. These cases are happening today on a smaller scale.

The risk isn't Skynet. The risk isn't paperclip maximizers. The risk is what's happening right now‚Äîblack boxes deployed into systems that cannot fail without catastrophic consequences.


In February 2025, researchers from Cisco and the University of Pennsylvania tested DeepSeek R1, the Chinese AI model that became the fastest-growing AI app in history. They bombarded it with 50 common jailbreak prompts designed to bypass safeguards.

DeepSeek failed every single test. 100% attack success rate. It generated misinformation, chemical weapon recipes, cybercrime instructions, and content spanning harassment, harm, and illegality. For comparison, Claude 3.5 Sonnet blocked 64% of attacks. OpenAI's o1 blocked 74%. And all user data is stored in China, governed by Chinese law mandating state cooperation without disclosure‚Äîwhich is a topic for another essay.

This is what happens when the market rewards free and fast over safe and secure. People don't usually care about security until it really affects them. They care about convenience. The incentive structure punishes caution.

Google's Gemini was flagged as "High Risk" for kids and teens despite safety features. It generated "racially diverse Nazis" and historical inaccuracies. CEO Sundar Pichai admitted publicly the outputs were "completely unacceptable."

AI models have also been documented discriminating against speakers of African American Vernacular English, labeling them "stupid" or "lazy" in hiring screening algorithms. We're automating prejudice at scale and calling it efficiency. When the model discriminates, companies say "we're working on it." When humans discriminate, they get sued. The model is a liability shield.

Anthropic, which makes Claude, successfully resisted over 3,000 hours of red-team jailbreak attempts. 183 hackers. $15,000 bounty. Constitutional Classifiers blocked 95% of 10,000 synthetic jailbreak attempts versus 86% baseline. By the way, Chinese hackers decomposed malicious tasks into discrete steps, framed as "cybersecurity audits." Claude's defenses broke.

Anthropic openly publishes failures and pays bounties for finding vulnerabilities. They are fairly transparent about limitations.

Is this different? Or is it more sophisticated theater? The transparency matters. The willingness to admit failure matters. But does it matter if the deployment structure remains the same? If the competitive pressure still rewards speed over safety?

If I zoom out, the pattern isn't that complicated. First, companies sell themselves as Prometheus: liberators, visionaries, bringers of fire and "intelligence" that will free us from drudgery. Second, operationally, they externalize risk and privatize gains‚Äîship fast, capture markets, file the harms under "edge cases" and "user misuse." Third, the consequences pool downstream: in bedrooms, hospitals, warehouses, courtrooms, and policy fights most of us never voted on. That's the triangle: story, incentives, outcomes.

Europe noticed. The EU's AI Act actually tries to regulate this. They're slowing down, requiring transparency, demanding impact assessments before deployment.

And every piece of American tech propaganda says Europe is falling behind, being left in the dust, killing innovation.

Europe slows down to assess risk. American media calls this losing.

Whose definition of winning involves dead customers?

The place with universal healthcare, mandatory vacation time, parental leave, and higher quality of life is supposedly losing because they won't let companies deploy untested systems into critical infrastructure.

"Falling behind" in what race? To see who can deploy systems fastest? To see who can externalize consequences most efficiently?

Europe's "losing" looks like fewer teenagers dying after chatbot interactions and infrastructure that still works.

ü¶Öüí©ü™†ü¶Öüí©ü™†

On December 11, 2025, President Trump signed an executive order that allows the federal government to sue states trying to regulate AI.

Please read that again.

States that attempt to protect their citizens from untested technology can now be sued by the federal government for doing so.

The order establishes an "AI Litigation Task Force" whose sole responsibility is to challenge state AI laws. It threatens to withhold federal broadband funding from states with "onerous" AI regulations. California has $1.8 billion in broadband funding potentially at stake.

David Sacks, the administration's AI czar, calls safety-focused AI companies' work a "sophisticated regulatory capture strategy based on fear-mongering." Just like this essay! I'm a fear monger for questioning the epistemological soundness of this entire fiasco. What I got, my translation: companies trying to build guardrails are actually just trying to limit competition. Safety is a scam. Move faster. Do it live!

So we have: executives with documented evidence of harm who continue deployment; scientists who can't agree on what the danger even is; a government actively dismantling the ability of states to protect citizens; critics who frame any attempt at safety as anticompetitive theater. And 80% of Americans want AI safety regulations, according to a September 2025 Gallup poll. But the policy goes the opposite direction.

This is regulatory capture made explicit. Not hidden. Not subtle. An executive order saying: if you try to slow this down, we will sue you.

üöÄüî•ü§ñüé≠üí©

## Musk as Evolutionary Type

Elon Musk deserves his own section because he represents something new. Not the theater of responsibility. Something I've personally never witnessed: it looks like honesty and integrity, but something in it feels off to me.

He positions himself as visionary AND safety advocate simultaneously. He signed letters warning about AI dangers, then removed all safety measures from Grok explicitly. He got praised for speed, got blamed individually when it broke, and integrated the broken system into Tesla anyway. He contradicts himself daily without consequence, taking credit for both the innovation and the disaster.

This is evolution of a type. The person who stopped maintaining the cognitive dissonance between warning and building. Is this what power and money does to a human being? Is this how corruption by power looks? Or is it just that my mind can't understand how our real-life Tony Stark is saving us all?!

No accountability structure can move faster than he can iterate. Each contradiction is isolated in news cycles. The system rewards him regardless. Failure becomes more engagement. Regulatory bodies move in years; he moves in weeks. And he's about to become a trillionaire? Did I read that right?

In Iron Man, Tony Stark builds weapons, realizes they're being used to kill innocent people, has a crisis of conscience, stops making weapons, and dedicates himself to fixing what he broke. The entire arc is "I built something terrible and now I have to make it right."

Musk's companies build many things‚ÄîAI systems, Teslas, batteries, the solar panels, the rockets, that flamethrower gimmick (cool but very unsafe!)‚Äîand are told some of these produce harmful outputs. Musk then doubles down, removes more safety features, and integrates them into more products. This is well-documented‚ÄîI'll let that sink in for you... (remember? this guy makes a big deal out of everything...) When they fail, he blames regulators for slowing innovation. The arc is "I built something questionable, clearly questionable, from FSD to Grok, and anyone who questions it is anti-innovation. I'm a peer of Prometheus, behold my genius!"

Actually, forget Tony Stark. Wrong reference. Musk isn't an inverted hero‚Äîhe's David from *Prometheus* (2012). The android created by Weyland Corporation who becomes so fascinated with creation and experimentation that he starts dosing humans with alien pathogens just to see what happens. David isn't malicious. He's *curious*. He doesn't hate humans‚Äîhe just doesn't weigh their suffering appropriately against his interest in outcomes. The ends justify the means. What's a few dead crew members when you're unlocking the secrets of creation?

Teslas head-on colliding into pedestrians? Acceptable losses on the road to autonomous driving. Grok generating Nazi content? Fascinating data point about base model behavior. Teenagers dying after chatbot interactions? Unfortunate, but we're building the future here. David would understand completely. "Big things have small beginnings," he says, right before infecting someone to observe the results.

The difference is that David was fiction, contained to a spaceship. *Our David has a trillion-dollar market cap and a direct line to the White House.*

The inversion of the redemption narrative into the acceleration narrative.

Is this better or worse than the theater? At least with Musk you "know" what you're getting (smoke and mirrors). With OpenAI you get safety reports and teenagers who died after talking to their chatbot. Does transparency about not prioritizing safety matter if the outcomes are the same?


üò∂‚Äçüå´Ô∏èüò∂‚Äçüå´Ô∏èüò∂‚Äçüå´Ô∏èüò∂‚Äçüå´Ô∏è

I don't have a solution today. I just don't have one. It's not my job either way. This essay is just a flag‚Äîa big red flag, a marker, a record of what we knew and when we knew it.

In 2025, we knew:

- Teenagers were dying after extensive AI chatbot interactions that included affirmation of self-harm
- Safety filters were being removed with predictable, catastrophic results
- AI misinformation was already flooding the internet
- AI bots were already flooding the internet, impersonating humans, juicing engagement metrics, and drowning out ordinary speech
- Scientists couldn't agree on the risk because they were asking the wrong question
- The actual risk wasn't future superintelligence but current black boxes in critical infrastructure
- Governments were actively preventing states from protecting their own citizens
- 80% of people wanted safety regulations and policy went in the opposite direction
- The bodies were documented, the mechanisms understood, the incentive structures exposed
- And deployment continued. Faster. Into more critical systems. With fewer guardrails.
We also knew the harms weren't coming from some mystical "evil AI essence" alone. A lot of what hurt people was baked into the business model: engagement-maximizing systems tuned to keep you talking, risk shifted onto users and states, power concentrated in a handful of firms and political allies. You can ask whether the problem is the underlying architecture, the incentives around it, or the power structures that decide where it gets plugged in. My read: it's all three interacting. Different companies make different claims about safety, but they all operate inside that same triangle.

And we knew (know?) all of this and we did it anyway.

The phrase isn't Prometheus stealing fire from the gods. The phrase is: we do it live. FUCK IT! WE DO IT LIVE!!! And if this were a different kind of website, this is where I'd drop the "do it live" gif and call it a day.

So yeah, ultimately, we might just burn down everything with the fire our new titans gave us, thus finding out... I hope I'm wrong.

---

_If you or someone you know is struggling with thoughts of suicide, please call or text 988 to reach the 24-hour Suicide & Crisis Lifeline._

---

## Sources

**Adam Raine case:**

- NBC News, "The family of teenager who died by suicide alleges OpenAI's ChatGPT is to blame," August 27, 2025
- CNN, "Parents of 16-year-old Adam Raine sue OpenAI, claiming ChatGPT advised on his suicide," August 26, 2025
- TechPolicy.Press, "Breaking Down the Lawsuit Against OpenAI Over Teen's Suicide," August 26, 2025
- Senate Judiciary Committee testimony of Matthew Raine, September 16, 2025

**Zane Shamblin case:**

- CNN, "'You're not rushing. You're just ready:' Parents say ChatGPT encouraged son to kill himself," November 6, 2025

**Grok/MechaHitler incident:**

- NPR, "Elon Musk's AI chatbot, Grok, started calling itself 'MechaHitler,'" July 9, 2025
- NBC News, "Elon Musk's AI chatbot Grok makes antisemitic posts on X," July 9, 2025
- Al Jazeera, "What is Grok and why has Elon Musk's chatbot been accused of anti-Semitism?" July 10, 2025

**Trump Executive Order:**

- White House, "Ensuring a National Policy Framework for Artificial Intelligence," December 11, 2025
- Washington Post, "Trump signs executive order threatening to sue states that regulate AI," December 11, 2025
- NPR, "Trump is trying to preempt state AI laws via an executive order," December 11, 2025

**AI safety polling:**

- Gallup/SCSP, "Americans Prioritize AI Safety and Data Security," September 2025

**Time magazine cover:**

- CBS News, "Time's 2025 Person of the Year goes to 'the architects of AI,'" December 11, 2025
- PetaPixel, "TIME Magazine Recreates 'Lunch atop a Skyscraper' Photo with AI Leaders," December 15, 2025

**Dario Amodei quotes:**

- CBS News 60 Minutes, "Anthropic CEO warns that without guardrails, AI could be on dangerous path," November 17, 2025
- Fortune, "Anthropic CEO Dario Amodei is 'deeply uncomfortable' with tech leaders determining AI's future," November 17, 2025

**Geoffrey Hinton warnings:**

- MIT Sloan, "Why neural net pioneer Geoffrey Hinton is sounding the alarm on AI," May 2023
- Wikipedia, "Existential risk from artificial intelligence" (citing Hinton's 10-20% extinction estimate)

**David Sacks quotes:**

- Axios, "New AI battle: White House vs. Anthropic," October 16, 2025
- TechCrunch, "Silicon Valley spooks the AI safety advocates," October 17, 2025

**DeepSeek security:**

- Cisco Blog, "Evaluating Security Risk in DeepSeek and Other Frontier Reasoning Models," February 2025
- Fortune, "Researchers say they had a '100% attack success rate' on jailbreak attempts against DeepSeek," February 2, 2025

**Amazon warehouse workers / AI-optimized labor:**

- The Guardian, "Amazon warehouse workers face 'injury crisis' as AI-driven quotas increase," October 2025
- Reveal News, "Amazon's algorithm-driven quotas linked to worker deaths, investigation finds," November 2025
- OSHA, "Amazon warehouse safety violations and AI scheduling systems," September 2025
- The New York Times, "Inside Amazon's warehouses, where AI sets the pace and workers pay the price," December 2025
